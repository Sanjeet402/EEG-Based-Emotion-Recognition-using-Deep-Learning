{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcfddd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 47.4409\n",
      "Epoch [2/50], Loss: 31.6503\n",
      "Epoch [3/50], Loss: 25.0713\n",
      "Epoch [4/50], Loss: 19.8801\n",
      "Epoch [5/50], Loss: 18.4690\n",
      "Epoch [6/50], Loss: 15.2755\n",
      "Epoch [7/50], Loss: 13.0785\n",
      "Epoch [8/50], Loss: 12.3072\n",
      "Epoch [9/50], Loss: 10.9445\n",
      "Epoch [10/50], Loss: 9.6125\n",
      "Epoch [11/50], Loss: 7.8980\n",
      "Epoch [12/50], Loss: 6.2396\n",
      "Epoch [13/50], Loss: 6.3804\n",
      "Epoch [14/50], Loss: 5.3582\n",
      "Epoch [15/50], Loss: 4.0992\n",
      "Epoch [16/50], Loss: 3.5946\n",
      "Epoch [17/50], Loss: 3.0887\n",
      "Epoch [18/50], Loss: 3.0847\n",
      "Epoch [19/50], Loss: 3.7791\n",
      "Epoch [20/50], Loss: 3.8239\n",
      "Epoch [21/50], Loss: 3.8036\n",
      "Epoch [22/50], Loss: 2.1579\n",
      "Epoch [23/50], Loss: 1.3307\n",
      "Epoch [24/50], Loss: 1.6894\n",
      "Epoch [25/50], Loss: 1.3495\n",
      "Epoch [26/50], Loss: 1.0611\n",
      "Epoch [27/50], Loss: 1.2763\n",
      "Epoch [28/50], Loss: 0.7897\n",
      "Epoch [29/50], Loss: 0.7817\n",
      "Epoch [30/50], Loss: 0.8607\n",
      "Epoch [31/50], Loss: 0.9403\n",
      "Epoch [32/50], Loss: 0.7058\n",
      "Epoch [33/50], Loss: 0.4972\n",
      "Epoch [34/50], Loss: 0.8531\n",
      "Epoch [35/50], Loss: 0.5045\n",
      "Epoch [36/50], Loss: 0.6199\n",
      "Epoch [37/50], Loss: 1.3023\n",
      "Epoch [38/50], Loss: 1.4813\n",
      "Epoch [39/50], Loss: 1.3202\n",
      "Epoch [40/50], Loss: 0.7398\n",
      "Epoch [41/50], Loss: 0.6730\n",
      "Epoch [42/50], Loss: 0.5105\n",
      "Epoch [43/50], Loss: 0.7508\n",
      "Epoch [44/50], Loss: 0.3316\n",
      "Epoch [45/50], Loss: 0.3959\n",
      "Epoch [46/50], Loss: 0.3150\n",
      "Epoch [47/50], Loss: 0.3274\n",
      "Epoch [48/50], Loss: 0.2833\n",
      "Epoch [49/50], Loss: 0.2352\n",
      "Epoch [50/50], Loss: 0.4804\n",
      "âœ… EEGNet+CBAM Training Complete!\n",
      "\n",
      "Accuracy: 0.8843\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8902    0.9241    0.9068        79\n",
      "           1     0.9531    0.8592    0.9037        71\n",
      "           2     0.8143    0.8636    0.8382        66\n",
      "\n",
      "    accuracy                         0.8843       216\n",
      "   macro avg     0.8859    0.8823    0.8829       216\n",
      "weighted avg     0.8877    0.8843    0.8848       216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Device Setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load SEED EEG Features ===\n",
    "DATA_PATH = \"D:/SEED_IV/SEED_IV/eeg_feature_smooth\"\n",
    "\n",
    "def pad_or_truncate(features, target_length):\n",
    "    current_length = len(features)\n",
    "    if current_length < target_length:\n",
    "        return np.pad(features, (0, target_length - current_length), mode='constant')\n",
    "    else:\n",
    "        return features[:target_length]\n",
    "\n",
    "def load_eeg_features(sessions=[\"1\", \"2\", \"3\"], feature_type=\"de_movingAve\", num_channels=62):\n",
    "    data = []\n",
    "    labels = []\n",
    "    max_length = 0\n",
    "\n",
    "    for session in sessions:\n",
    "        session_path = os.path.join(DATA_PATH, session)\n",
    "        if not os.path.exists(session_path):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(session_path):\n",
    "            if not file.endswith(\".mat\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(session_path, file)\n",
    "            mat = scipy.io.loadmat(file_path)\n",
    "\n",
    "            for trial in range(1, 25):\n",
    "                key = f\"{feature_type}{trial}\"\n",
    "                if key not in mat:\n",
    "                    continue\n",
    "\n",
    "                feature_matrix = mat[key]\n",
    "                if feature_matrix.shape[0] != num_channels:\n",
    "                    continue\n",
    "\n",
    "                trial_features = feature_matrix.reshape(-1)\n",
    "                max_length = max(max_length, len(trial_features))\n",
    "                data.append(trial_features)\n",
    "                labels.append((trial - 1) % 3)\n",
    "\n",
    "    if not data:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    data_fixed = np.array([pad_or_truncate(x, max_length) for x in data])\n",
    "    return np.array(data_fixed), np.array(labels)\n",
    "\n",
    "# === Load & Normalize Data ===\n",
    "X_fixed, y = load_eeg_features()\n",
    "scaler = StandardScaler()\n",
    "X_fixed = scaler.fit_transform(X_fixed)\n",
    "\n",
    "X_tensor = torch.tensor(X_fixed, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# === Train-Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# === CBAM Model ===\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 2, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // 2, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        channel_attention = self.channel_attention(x)\n",
    "        x = x * channel_attention\n",
    "        \n",
    "        # Spatial attention\n",
    "        spatial_attention = self.spatial_attention(x)\n",
    "        x = x * spatial_attention\n",
    "        \n",
    "        return x\n",
    "\n",
    "class EEGNet_CBAM(nn.Module):\n",
    "    def __init__(self, num_classes=3, input_channels=62, input_samples=320):\n",
    "        super(EEGNet_CBAM, self).__init__()\n",
    "\n",
    "        # Temporal convolution\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=(1, 64), padding=(0, 32), bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        # Depthwise convolution\n",
    "        self.depthwise_conv = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=(input_channels, 1), groups=16, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        # Separable convolution\n",
    "        self.separable_conv = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(1, 16), padding=(0, 8), bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        # CBAM module\n",
    "        self.cbam = CBAM(in_channels=64)\n",
    "\n",
    "        # Calculate the output size after the convolution layers\n",
    "        self._to_linear = None\n",
    "        self.convs = nn.Sequential(\n",
    "            self.temporal_conv,\n",
    "            self.depthwise_conv,\n",
    "            self.separable_conv,\n",
    "            self.cbam\n",
    "        )\n",
    "        self._calc_linear_size(input_samples)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.input_samples = input_samples\n",
    "\n",
    "    def _calc_linear_size(self, input_samples):\n",
    "        # Create a dummy input to calculate the output size after convolutions\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.ones(1, 1, 62, input_samples)\n",
    "            x = self.convs(dummy_input)\n",
    "            self._to_linear = int(np.prod(x.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the convolution layers\n",
    "        x = self.convs(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# === Model Initialization ===\n",
    "model = EEGNet_CBAM(num_classes=3, input_channels=62, input_samples=320).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# === Training Loop ===\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        batch_X = batch_X.view(-1, 1, 62, 320)  # Reshape for the model\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
    "\n",
    "print(\"âœ… EEGNet+CBAM Training Complete!\")\n",
    "\n",
    "# === Evaluation ===\n",
    "model.eval()\n",
    "all_preds, all_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        batch_X = batch_X.view(-1, 1, 62, 320)  # Reshape for the model\n",
    "\n",
    "        outputs = model(batch_X)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(all_targets, all_preds)\n",
    "report = classification_report(all_targets, all_preds, digits=4)\n",
    "print(f\"\\nAccuracy: {acc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2663609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Region-wise Accuracy (CBAM):\n",
      "Frontal: 0.8009\n",
      "Temporal: 0.5676\n",
      "Parietal: 0.5972\n",
      "Occipital: 0.3880\n",
      "Central: 0.3806\n"
     ]
    }
   ],
   "source": [
    "def region_wise_analysis_cbam(model, X_tensor, y_tensor, region_channels_dict, total_channels=62, input_samples=320):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "\n",
    "    for region, channels in region_channels_dict.items():\n",
    "        X_region = X_tensor[:, channels, :]  # Extract region-specific channels\n",
    "\n",
    "        # Pad to 62 channels with zeros for unused channels\n",
    "        padded = torch.zeros((X_region.shape[0], total_channels, input_samples))\n",
    "        padded[:, channels, :] = X_region\n",
    "\n",
    "        # Reshape for model input\n",
    "        padded = padded.unsqueeze(1).to(device)  # Shape: [N, 1, 62, 320]\n",
    "        y_tensor_device = y_tensor.to(device)\n",
    "\n",
    "        # Evaluate region-specific input\n",
    "        with torch.no_grad():\n",
    "            outputs = model(padded)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            acc = accuracy_score(y_tensor_device.cpu().numpy(), preds.cpu().numpy())\n",
    "\n",
    "        results[region] = acc\n",
    "\n",
    "    return results\n",
    "\n",
    "# === Define Region Mapping (adjust indices as per EEG cap layout if needed) ===\n",
    "region_channels = {\n",
    "    'Frontal': list(range(0, 20)),      # Approximate\n",
    "    'Temporal': list(range(20, 30)),\n",
    "    'Parietal': list(range(30, 45)),\n",
    "    'Occipital': list(range(45, 52)),\n",
    "    'Central': list(range(52, 62))\n",
    "}\n",
    "\n",
    "# === Run Region-wise Analysis ===\n",
    "region_scores_cbam = region_wise_analysis_cbam(model, X_tensor.view(-1, 62, 320), y_tensor, region_channels)\n",
    "\n",
    "print(\"\\nðŸ§  Region-wise Accuracy (CBAM):\")\n",
    "for region, score in region_scores_cbam.items():\n",
    "    print(f\"{region}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed66502c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "EEGNet_CBAM                              [16, 3]                   --\n",
       "â”œâ”€Sequential: 1-1                        [16, 64, 1, 322]          --\n",
       "â”‚    â””â”€Sequential: 2-1                   [16, 16, 62, 321]         --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-1                  [16, 16, 62, 321]         1,024\n",
       "â”‚    â”‚    â””â”€BatchNorm2d: 3-2             [16, 16, 62, 321]         32\n",
       "â”‚    â”‚    â””â”€ELU: 3-3                     [16, 16, 62, 321]         --\n",
       "â”‚    â”‚    â””â”€Dropout: 3-4                 [16, 16, 62, 321]         --\n",
       "â”‚    â””â”€Sequential: 2-2                   [16, 32, 1, 321]          --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-5                  [16, 32, 1, 321]          1,984\n",
       "â”‚    â”‚    â””â”€BatchNorm2d: 3-6             [16, 32, 1, 321]          64\n",
       "â”‚    â”‚    â””â”€ELU: 3-7                     [16, 32, 1, 321]          --\n",
       "â”‚    â”‚    â””â”€Dropout: 3-8                 [16, 32, 1, 321]          --\n",
       "â”‚    â””â”€Sequential: 2-3                   [16, 64, 1, 322]          --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-9                  [16, 64, 1, 322]          32,768\n",
       "â”‚    â”‚    â””â”€BatchNorm2d: 3-10            [16, 64, 1, 322]          128\n",
       "â”‚    â”‚    â””â”€ELU: 3-11                    [16, 64, 1, 322]          --\n",
       "â”‚    â”‚    â””â”€Dropout: 3-12                [16, 64, 1, 322]          --\n",
       "â”‚    â””â”€CBAM: 2-4                         [16, 64, 1, 322]          --\n",
       "â”‚    â”‚    â””â”€Sequential: 3-13             [16, 64, 1, 322]          4,192\n",
       "â”‚    â”‚    â””â”€Sequential: 3-14             [16, 1, 1, 322]           3,137\n",
       "â”œâ”€Linear: 1-2                            [16, 128]                 2,637,952\n",
       "â”œâ”€Linear: 1-3                            [16, 3]                   387\n",
       "==========================================================================================\n",
       "Total params: 2,681,668\n",
       "Trainable params: 2,681,668\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 585.06\n",
       "==========================================================================================\n",
       "Input size (MB): 1.27\n",
       "Forward/backward pass size (MB): 93.44\n",
       "Params size (MB): 10.73\n",
       "Estimated Total Size (MB): 105.44\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# EEGNet_CBAM expects input shape: [batch, 1, 62, 320]\n",
    "summary(model, input_size=(16, 1, 62, 320))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c1516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
